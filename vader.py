from FunctionExtract import core_extractor
from transformers import RobertaTokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer
import inspect
import sys
import typer
from pathlib import Path
from rich import print as pprint, console
from rich.progress import Progress, SpinnerColumn, TextColumn, track
import pandas as pd
import os
from multishot import multi_shot_primer, multi_shot_comment, multi_shot_comment_end, multi_shot_key
import torch

def parse_df_to_dict(code_dataframe: pd.DataFrame):
    """ Function Makes the Pandas DataFrame into something more parsable
        @parameters
        code_dataframe: the dataframe generated by the extractor
        @return
        This function returns a dictionary where the keys are the filepaths and the values are dictionaries of
        the code and line numbers
    """
    parsed_data = {}
    for file_id, code, line in zip(code_dataframe["Uniq ID"], code_dataframe["Code"], code_dataframe["Line"]):
        og_file_name = file_id[:file_id.find("_", file_id.rfind("."))]
        if og_file_name in parsed_data:
            parsed_data[og_file_name].append({"code":code, "line_no":line})
        else:
            parsed_data[og_file_name] = [{"code":code, "line_no":line}]
    return parsed_data

#languages
COMMENT_MAP = {
    ".PY": "#",
    ".CPP": "//",
    ".JAVA": "//"
}


def main(
        directory: Path = typer.Argument(..., help="Directory of Source code to Parse"),
        ignore_documented: bool = typer.Option(False, "--ignore-documented", help = "Default False; ignores documented functions"),
        remove_cpp_signatures: bool = typer.Option(False, "--remove-cpp-signatures", help = "Default False; removes signatures of C++ functions before processing"),
        overwrite_files: bool = typer.Option(False, "--overwrite-files", help = "Default False; overwrites original files with generated comments instead of creating new ones"),
        non_recursive: bool = typer.Option(False, "--non-recursive", help = "Default False; only generate comments for files in immediate directory and not children directories"),
        verbose: bool = typer.Option(False, "--verbose", help = "Default False; display verbose output during program execution"),
        new_directories: bool = typer.Option(False, "--new-directories", help = "Default False; creates new directories within which to put code with generated comments"),
        use_cuda: bool = typer.Option(False, "--cuda", help="Default False; uses nvidia gpu for inference. Make sure appropriate drivers/libraries are installed."),
        custom_t5_model: str = typer.Option("", "--custom-t5-model", help="Default T5-Base; customize T5 model used for inference"),
        custom_gpt2_model: str = typer.Option("", "--custom-gpt2-model", help="Default None; customize gpt2 model used for inference"),
        gpt2_style: str = typer.Option("DOCSTYLE", "--gpt2-style", help="Default DOCSTYLE; Change style on Comments for gpt2 models")
        ):
    
    if not directory.is_dir():
        pprint("[bold red]Must be a directory[/bold red]")
        raise typer.Exit()
    
    if(new_directories and overwrite_files):
        pprint("The new-directories flag and the overwrite-files flags are mutually exclusive. [bold red]Please only specify one.[/bold red]")
        raise typer.Exit()
    

    # Use GPU
    device = None
    if use_cuda:
        if torch.cuda.is_available():
            device = torch.device('cuda')
            pprint(f"Using [bold green]{device}[/bold green] for inference")
        else:
            device =torch.device('cpu')
            pprint(f"[bold red]Did not find cuda device.[/bold red] Using [bold green]{device}[/bold green] for inference")
    else:
        device = torch.device('cpu')
        pprint(f"Using [bold green]{device}[/bold green] for inference")

    # Code Extracting
    code_info_df = None
    
    # Model Collision
    vader_path = os.path.dirname(os.path.abspath(__file__))
    cache_dir = os.path.join(vader_path, "models")
    model_path = None
    token_path = None
    if custom_gpt2_model and custom_t5_model:
        pprint("custom-gpt2-model and custom-t5-model are mutually exclusive. [bold red]Please only specify one.[/bold red]")
        raise typer.Exit()
    elif custom_t5_model == "" and custom_gpt2_model == "":
        if os.path.exists(os.path.join(vader_path, "models/custom_t5/pytorch_model.bin")):
            pprint("Using [bold green]Custom Vader Model[/bold green] for inference")
            model_path = os.path.join(vader_path, "models/custom_t5")
            token_path = os.path.join(vader_path, "models/custom_t5")
        else:
            pprint("Using [bold green]Stock CodeT5 Model[/bold green] for inference")
            model_path = "Salesforce/codet5-base-multi-sum"
            token_path = "Salesforce/codet5-base-multi-sum"
    elif custom_t5_model:
        pprint(f"Using [bold green]{custom_t5_model}[/bold green] for inference")
        model_path = custom_t5_model
        token_path = custom_t5_model
    else:
        pprint(f"Using [bold green]{custom_gpt2_model}[/bold green] for inference")
        model_path = custom_gpt2_model
        token_path = custom_gpt2_model
            

    # Transform Data
    parsed_dict = {}

    pprint(f"Generating comments for [bold green]{directory}[/bold green]")

    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"))  as progress_parsing:
        progress_parsing.add_task(description="Extracting Functions", total=None)
        progress_parsing.add_task(description="Parsing Functions", total=None)
        code_info_df = core_extractor.extractor(directory, ignoreDocumented = ignore_documented, removeCppSignatures = remove_cpp_signatures, non_recursive = non_recursive, verbose=verbose)
        parsed_dict = parse_df_to_dict(code_info_df)

    pprint(f"Found [bold green]{code_info_df.shape[0]}[/bold green] functions in [bold green]{len(parsed_dict.keys())}[/bold green] files")


    # Retrieve Model
    tokenizer = None
    model = None

    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), redirect_stdout=verbose)  as progress_model:
        progress_model.add_task(description="Setting up Model (This may take a while)", total=None)
        try:
            if custom_gpt2_model:
                model = AutoModelForCausalLM.from_pretrained(model_path,cache_dir=cache_dir, torch_dtype=torch.float16).to(device)
                tokenizer = AutoTokenizer.from_pretrained(token_path, cache_dir=cache_dir)
            else:
                model = T5ForConditionalGeneration.from_pretrained(model_path,cache_dir=cache_dir).to(device)
                tokenizer = RobertaTokenizer.from_pretrained(token_path, cache_dir=cache_dir)
        except ValueError as e:
            pprint("Model was not [bold red]Valid[/bold red]")
            raise typer.Exit()
    

    # Inference
    for key in track(parsed_dict.keys(), "Generating Comments..."):
        for index, code_info in enumerate(parsed_dict[key]):
            if custom_gpt2_model:
                primed_code = multi_shot_primer[gpt2_style]+code_info["code"][:1250]+multi_shot_comment[gpt2_style]
                input_ids = tokenizer.encode(primed_code, return_tensors="pt").to(device)
                generated_ids = model.generate(input_ids, max_new_tokens=300)
                generated_comment = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                generated_comment = generated_comment[generated_comment.find(multi_shot_key[gpt2_style]):]
                print(generated_comment)
                generated_comment = generated_comment[generated_comment.find(multi_shot_comment[gpt2_style])+len(multi_shot_comment[gpt2_style]):]
                generated_comment = generated_comment[:generated_comment.find(multi_shot_comment_end[gpt2_style])]
                parsed_dict[key][index]["generated_comment"] = generated_comment
                del input_ids
            else:
                input_ids = tokenizer(code_info["code"][:512], return_tensors="pt").input_ids.to(device)
                generated_ids = model.generate(input_ids, max_length=512)
                parsed_dict[key][index]["generated_comment"] = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            if(verbose): pprint(f"Comment generated for " + key) 

    # Saving File
    for key in track(parsed_dict.keys(), "Saving Comments..."):
        parsed_dict[key].sort(key=lambda x: x["line_no"])
        if(new_directories):
            if (not (os.path.isdir(key[:key.rfind("/")] + "/VaderSC_Commented"))):
                os.mkdir(key[:key.rfind("/")] + "/VaderSC_Commented")
            mod_file_name = key[:key.rfind("/")] + "/VaderSC_Commented" + key[key.rfind("/") :]
            mod_file_name = key[:key.rfind("/")] + "/VaderSC_Commented" + key[key.rfind("/") :]
        else:
            mod_file_name = key[:key.rfind(".")] + "_mod" + key[key.rfind("."):]
        line_counter = 1
        array_counter = 0
        if (os.path.isfile(key)):
            with open(key, "r") as og_file:
                with open(mod_file_name, "w") as mod_file:
                    while(array_counter != len(parsed_dict[key])):
                        file_ending = Path(key).suffix.upper()
                        if(parsed_dict[key][array_counter]["line_no"] == line_counter):
                            mod_file.write(f"{COMMENT_MAP[file_ending]} Generated: {parsed_dict[key][array_counter]['generated_comment']}\n")
                            array_counter +=1
                        else:
                            mod_file.write(og_file.readline())
                            line_counter +=1
                    mod_file.write(og_file.read())
                    if(verbose): pprint(f"Comment inserted for " + key)
            if(overwrite_files):
                os.remove(key)
                os.rename(mod_file_name, key)
    
    pprint(f"Created [bold green]{len(parsed_dict)}[/bold green] files")


if __name__ == '__main__':
    typer.run(main)
